{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Build a Contextual Retrieval based RAG System"],"metadata":{"id":"jbw4wHV4zlKj"}},{"cell_type":"markdown","source":["## Install OpenAI, and LangChain dependencies"],"metadata":{"id":"4vtFl39Ofu_8"}},{"cell_type":"code","source":["!pip install langchain==0.3.26\n","!pip install langchain-openai==0.3.28\n","!pip install langchain-community==0.3.27\n","!pip install jq==1.9.1\n","!pip install pymupdf==1.26.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b8e9951-a515-4b93-c28f-d74228e807e7","id":"LVX6450Lfu_9","executionInfo":{"status":"ok","timestamp":1752496761847,"user_tz":-330,"elapsed":28785,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain==0.3.26 in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.26) (0.3.68)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.26) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.26) (0.4.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.26) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.26) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.26) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.26) (6.0.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain==0.3.26) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain==0.3.26) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain==0.3.26) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain==0.3.26) (4.14.1)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.26) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.26) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.26) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.26) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.26) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.26) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.26) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.26) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.26) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.26) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.26) (2025.7.9)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.26) (3.2.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.26) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.26) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.26) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain==0.3.26) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.26) (1.3.1)\n","Requirement already satisfied: langchain-openai==0.3.28 in /usr/local/lib/python3.11/dist-packages (0.3.28)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.28) (0.3.68)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.28) (1.93.3)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.28) (0.9.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (0.4.4)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (4.14.1)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (2.11.7)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (4.67.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.28) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.28) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (2025.7.9)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai==0.3.28) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.28) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.28) (2.4.0)\n","Requirement already satisfied: langchain-community==0.3.27 in /usr/local/lib/python3.11/dist-packages (0.3.27)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (0.3.68)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (0.3.26)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (2.10.1)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (0.4.4)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.27) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27) (0.9.0)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (4.14.1)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (0.23.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.27) (1.1.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.27) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (2025.7.9)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.27) (3.2.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (2.33.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27) (1.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (1.3.1)\n","Requirement already satisfied: jq==1.9.1 in /usr/local/lib/python3.11/dist-packages (1.9.1)\n","Requirement already satisfied: pymupdf==1.26.3 in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"]}]},{"cell_type":"markdown","source":["## Install Chroma Vector DB and LangChain wrapper"],"metadata":{"id":"bwUBYHjPfu_-"}},{"cell_type":"code","source":["!pip install langchain-chroma==0.2.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a587d16-b353-4473-f3c1-82fc56291a0b","id":"p30SmCgTfu__","executionInfo":{"status":"ok","timestamp":1752496775704,"user_tz":-330,"elapsed":13854,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain-chroma==0.2.4 in /usr/local/lib/python3.11/dist-packages (0.2.4)\n","Requirement already satisfied: langchain-core>=0.3.60 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma==0.2.4) (0.3.68)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma==0.2.4) (2.0.2)\n","Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma==0.2.4) (1.0.15)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (1.2.2.post1)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (2.11.7)\n","Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (1.4.1)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.35.0)\n","Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (5.4.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (4.14.1)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (1.22.1)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (1.35.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (1.35.0)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (1.35.0)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (0.21.2)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (4.67.1)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (1.73.1)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (4.3.0)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (0.16.0)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (33.1.0)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (8.5.0)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (6.0.2)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (5.1.0)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (3.10.18)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma==0.2.4) (4.24.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain-chroma==0.2.4) (0.4.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain-chroma==0.2.4) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain-chroma==0.2.4) (24.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2025.7.9)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.60->langchain-chroma==0.2.4) (3.0.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.26.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.9.0.post0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.38.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (3.3.1)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.4.0)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.10)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.60->langchain-chroma==0.2.4) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.60->langchain-chroma==0.2.4) (0.23.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma==0.2.4) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma==0.2.4) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma==0.2.4) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.13.1)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (8.7.0)\n","Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.70.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.35.0)\n","Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.35.0)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.56b0)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.2.1)\n","Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.9.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.4.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (2.19.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.33.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.5.4)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.1.1)\n","Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.21.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.1.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma==0.2.4) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (4.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma==0.2.4) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma==0.2.4) (2025.3.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.1.5)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (3.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (3.4.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.3.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma==0.2.4) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma==0.2.4) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma==0.2.4) (0.6.1)\n"]}]},{"cell_type":"markdown","source":["## Enter Open AI API Key"],"metadata":{"id":"EITC17hwfu__"}},{"cell_type":"code","source":["n\n","from getpass import getpass\n","\n","OPENAI_KEY = getpass('Enter Open AI API Key: ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"51aa8aca-b0f4-449a-d820-a3ee9b2b302a","id":"yEh2olNvfvAA","executionInfo":{"status":"ok","timestamp":1752496780400,"user_tz":-330,"elapsed":4693,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Open AI API Key: ··········\n"]}]},{"cell_type":"markdown","source":["## Setup Environment Variables"],"metadata":{"id":"pm_mx0v-fvAA"}},{"cell_type":"code","source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = OPENAI_KEY"],"metadata":{"id":"Jhfb4gMUfvAC","executionInfo":{"status":"ok","timestamp":1752496780447,"user_tz":-330,"elapsed":43,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### Open AI Embedding Models\n","\n","LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."],"metadata":{"id":"jiokYxD8fvAC"}},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n","openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"],"metadata":{"id":"-On4AS0HfvAD","executionInfo":{"status":"ok","timestamp":1752496780755,"user_tz":-330,"elapsed":287,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["## Loading and Processing the Data"],"metadata":{"id":"afzeN_WkHIz2"}},{"cell_type":"markdown","source":["### Get the dataset"],"metadata":{"id":"RA_-hzHbFeSP"}},{"cell_type":"code","source":["# if you can't download using the following code\n","# go to https://drive.google.com/file/d/1aZxZejfteVuofISodUrY2CDoyuPLYDGZ download it\n","# manually upload it on colab\n","!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"],"metadata":{"id":"RZFMYH-yFhWn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7698f36a-baba-4eaf-c4d6-8f10c1d6825e","executionInfo":{"status":"ok","timestamp":1752496785908,"user_tz":-330,"elapsed":5150,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1aZxZejfteVuofISodUrY2CDoyuPLYDGZ\n","To: /content/rag_docs.zip\n","100% 5.92M/5.92M [00:00<00:00, 39.3MB/s]\n"]}]},{"cell_type":"code","source":["!unzip rag_docs.zip"],"metadata":{"id":"WwLEBC4nF9ly","colab":{"base_uri":"https://localhost:8080/"},"outputId":"36d5b681-8e74-48d0-90a6-36e6cbbf2a18","executionInfo":{"status":"ok","timestamp":1752497067413,"user_tz":-330,"elapsed":281482,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  rag_docs.zip\n","replace rag_docs/attention_paper.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace rag_docs/cnn_paper.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace rag_docs/resnet_paper.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace rag_docs/vision_transformer.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace rag_docs/wikidata_rag_demo.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"]}]},{"cell_type":"markdown","source":["### Load and Process JSON Documents"],"metadata":{"id":"wMlxKZ_5jIdE"}},{"cell_type":"code","source":["from langchain.document_loaders import JSONLoader\n","\n","loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n","                    jq_schema='.',\n","                    text_content=False,\n","                    json_lines=True)\n","wiki_docs = loader.load()"],"metadata":{"id":"RZ5y0NfzHPhg","executionInfo":{"status":"ok","timestamp":1752497067489,"user_tz":-330,"elapsed":72,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["len(wiki_docs)"],"metadata":{"id":"G4E1zYFSG7J-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"51cd1d1b-66d5-472c-ca4b-6f7a24edc6d0","executionInfo":{"status":"ok","timestamp":1752497067505,"user_tz":-330,"elapsed":13,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1801"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["wiki_docs[3]"],"metadata":{"id":"aSbhERAyGw0v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06d58fae-3fac-432f-b780-01c055f3cbe6","executionInfo":{"status":"ok","timestamp":1752497067590,"user_tz":-330,"elapsed":44,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 4}, page_content='{\"id\": \"71548\", \"title\": \"Chi-square distribution\", \"paragraphs\": [\"In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\\\u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution.\", \"Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\"]}')"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import json\n","from langchain.docstore.document import Document\n","wiki_docs_processed = []\n","\n","for doc in wiki_docs:\n","    doc = json.loads(doc.page_content)\n","    metadata = {\n","        \"title\": doc['title'],\n","        \"id\": doc['id'],\n","        \"source\": \"Wikipedia\",\n","        \"page\": 1\n","    }\n","    data = ' '.join(doc['paragraphs'])\n","    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"],"metadata":{"id":"yICyAF85h2DO","executionInfo":{"status":"ok","timestamp":1752497067590,"user_tz":-330,"elapsed":30,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["wiki_docs_processed[3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IATrHWKh7II","outputId":"71f8b66d-454d-4ae7-d5d2-3edc807327cf","executionInfo":{"status":"ok","timestamp":1752497067591,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'title': 'Chi-square distribution', 'id': '71548', 'source': 'Wikipedia', 'page': 1}, page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.')"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### Load and Process PDF documents"],"metadata":{"id":"F_GzvHP1jSBo"}},{"cell_type":"markdown","source":["#### Create Chunk Contexts for Contextual Retrieval\n","\n","![](https://i.imgur.com/LRhKHzk.png)"],"metadata":{"id":"4vH6xGFOnv7m"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"],"metadata":{"id":"jxHHyhlbl_9j","executionInfo":{"status":"ok","timestamp":1752497067592,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# create chunk context generation chain\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import StrOutputParser\n","\n","\n","def generate_chunk_context(document, chunk):\n","\n","    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n","                            Your task is to provide brief, relevant context for a chunk of text\n","                            based on the following research paper.\n","\n","                            Here is the research paper:\n","                            <paper>\n","                            {paper}\n","                            </paper>\n","\n","                            Here is the chunk we want to situate within the whole document:\n","                            <chunk>\n","                            {chunk}\n","                            </chunk>\n","\n","                            Provide a concise context (3-4 sentences max) for this chunk,\n","                            considering the following guidelines:\n","\n","                            - Give a short succinct context to situate this chunk within the overall document\n","                            for the purposes of improving search retrieval of the chunk.\n","                            - Answer only with the succinct context and nothing else.\n","                            - Context should be mentioned like 'Focuses on ....'\n","                            do not mention 'this chunk or section focuses on...'\n","\n","                            Context:\n","                        \"\"\"\n","\n","    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n","\n","    agentic_chunk_chain = (prompt_template\n","                                |\n","                            chatgpt\n","                                |\n","                            StrOutputParser())\n","\n","    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n","\n","    return context"],"metadata":{"id":"MHSh0Vg-mIUv","executionInfo":{"status":"ok","timestamp":1752497067632,"user_tz":-330,"elapsed":38,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import PyMuPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import uuid\n","\n","def create_contextual_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n","\n","    print('Loading pages:', file_path)\n","    loader = PyMuPDFLoader(file_path)\n","    doc_pages = loader.load()\n","\n","    print('Chunking pages:', file_path)\n","    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n","                                              chunk_overlap=chunk_overlap)\n","    doc_chunks = splitter.split_documents(doc_pages)\n","\n","    print('Generating contextual chunks:', file_path)\n","    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n","    contextual_chunks = []\n","    for chunk in doc_chunks:\n","        chunk_content = chunk.page_content\n","        chunk_metadata = chunk.metadata\n","        chunk_metadata_upd = {\n","            'id': str(uuid.uuid4()),\n","            'page': chunk_metadata['page'],\n","            'source': chunk_metadata['source'],\n","            'title': chunk_metadata['source'].split('/')[-1]\n","        }\n","        context = generate_chunk_context(original_doc, chunk_content)\n","        contextual_chunks.append(Document(page_content=context+'\\n'+chunk_content,\n","                                          metadata=chunk_metadata_upd))\n","    print('Finished processing:', file_path)\n","    print()\n","    return contextual_chunks"],"metadata":{"id":"-uxOSfcsxqHD","executionInfo":{"status":"ok","timestamp":1752497067637,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["from glob import glob\n","\n","pdf_files = glob('./rag_docs/*.pdf')\n","pdf_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-VUpdmczt0C","outputId":"815b3021-5345-4124-d22b-df7e92da7401","executionInfo":{"status":"ok","timestamp":1752497067646,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./rag_docs/attention_paper.pdf',\n"," './rag_docs/resnet_paper.pdf',\n"," './rag_docs/cnn_paper.pdf',\n"," './rag_docs/vision_transformer.pdf']"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["paper_docs = []\n","for fp in pdf_files:\n","    paper_docs.extend(create_contextual_chunks(file_path=fp, chunk_size=3500))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsicBMPazzlg","outputId":"397cf384-d2e3-4503-a962-4c059c1c9a5c","executionInfo":{"status":"ok","timestamp":1752497291293,"user_tz":-330,"elapsed":223645,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pages: ./rag_docs/attention_paper.pdf\n","Chunking pages: ./rag_docs/attention_paper.pdf\n","Generating contextual chunks: ./rag_docs/attention_paper.pdf\n","Finished processing: ./rag_docs/attention_paper.pdf\n","\n","Loading pages: ./rag_docs/resnet_paper.pdf\n","Chunking pages: ./rag_docs/resnet_paper.pdf\n","Generating contextual chunks: ./rag_docs/resnet_paper.pdf\n","Finished processing: ./rag_docs/resnet_paper.pdf\n","\n","Loading pages: ./rag_docs/cnn_paper.pdf\n","Chunking pages: ./rag_docs/cnn_paper.pdf\n","Generating contextual chunks: ./rag_docs/cnn_paper.pdf\n","Finished processing: ./rag_docs/cnn_paper.pdf\n","\n","Loading pages: ./rag_docs/vision_transformer.pdf\n","Chunking pages: ./rag_docs/vision_transformer.pdf\n","Generating contextual chunks: ./rag_docs/vision_transformer.pdf\n","Finished processing: ./rag_docs/vision_transformer.pdf\n","\n"]}]},{"cell_type":"code","source":["len(paper_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOkwPMxp0gFh","outputId":"0fde0c86-a3ea-4715-9f6f-0e87905b8de4","executionInfo":{"status":"ok","timestamp":1752497291312,"user_tz":-330,"elapsed":17,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["79"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["paper_docs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVK2i0lR0ieV","outputId":"11e98509-3f49-4a23-e0c9-d60e9e295bff","executionInfo":{"status":"ok","timestamp":1752497291377,"user_tz":-330,"elapsed":63,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'id': '52389109-3a96-4198-b9c6-76e3b207f8b8', 'page': 0, 'source': './rag_docs/attention_paper.pdf', 'title': 'attention_paper.pdf'}, page_content='Focuses on the authorship and permissions related to the research paper \"Attention Is All You Need,\" which introduces the Transformer model for sequence transduction tasks. It highlights the contributions of various authors from Google Brain and Google Research, as well as the paper\\'s presentation at the 31st Conference on Neural Information Processing Systems (NIPS 2017).\\nProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["### Combine all document chunks in one list"],"metadata":{"id":"UyPdlZo2xEly"}},{"cell_type":"code","source":["len(wiki_docs_processed)"],"metadata":{"id":"UbtpR-r50mEn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5825c17-6cad-4d11-9db6-93c909ecb4b1","executionInfo":{"status":"ok","timestamp":1752497291393,"user_tz":-330,"elapsed":14,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1801"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["total_docs = wiki_docs_processed + paper_docs\n","len(total_docs)"],"metadata":{"id":"lNQWgq9t0pMH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1ec290e8-0fdd-4bc6-d8b8-e6ef450a5768","executionInfo":{"status":"ok","timestamp":1752497291405,"user_tz":-330,"elapsed":11,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1880"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["## Index Document Chunks and Embeddings in Vector DB\n","\n","Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."],"metadata":{"id":"Daqn6Hglw9Nk"}},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","# create vector DB of docs and embeddings - takes < 30s on Colab\n","chroma_db = Chroma.from_documents(documents=total_docs,\n","                                  collection_name='my_context_db',\n","                                  embedding=openai_embed_model,\n","                                  # need to set the distance function to cosine else it uses euclidean by default\n","                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n","                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n","                                  persist_directory=\"./my_context_db\")"],"metadata":{"id":"ZhAQyrFBfvAN","executionInfo":{"status":"ok","timestamp":1752497311213,"user_tz":-330,"elapsed":19807,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ju_zBIj1Zsb"},"source":["### Load Vector DB from disk\n","\n","This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"pNvj0dDH1WDg","executionInfo":{"status":"ok","timestamp":1752497311228,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"outputs":[],"source":["# load from disk\n","chroma_db = Chroma(persist_directory=\"./my_context_db\",\n","                   collection_name='my_context_db',\n","                   embedding_function=openai_embed_model)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"NFC3uPqYop0a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b315287-8300-41dc-e38b-d552299ce236","executionInfo":{"status":"ok","timestamp":1752497311249,"user_tz":-330,"elapsed":14,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain_chroma.vectorstores.Chroma at 0x789da0188890>"]},"metadata":{},"execution_count":41}],"source":["chroma_db"]},{"cell_type":"markdown","source":["### Semantic Similarity based Retrieval\n","\n","We use simple cosine similarity here and retrieve the top 5 similar documents based on the user input query"],"metadata":{"id":"njfZOOVZxj1a"}},{"cell_type":"code","source":["similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n","                                              search_kwargs={\"k\": 5})"],"metadata":{"id":"tV1l6HYdxj1b","executionInfo":{"status":"ok","timestamp":1752497311252,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_docs(docs):\n","    for doc in docs:\n","        print('Metadata:', doc.metadata)\n","        print('Content Brief:')\n","        display(Markdown(doc.page_content[:1000]))\n","        print()"],"metadata":{"id":"nUIJG_bDxj1c","executionInfo":{"status":"ok","timestamp":1752497311255,"user_tz":-330,"elapsed":1,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["query = \"what is machine learning?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":948},"id":"PIh4xGv2xj1c","outputId":"07027991-a92e-4d90-c9a5-9a76f505294c","executionInfo":{"status":"ok","timestamp":1752497312626,"user_tz":-330,"elapsed":1369,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'title': 'Machine learning', 'id': '564928', 'page': 1, 'source': 'Wikipedia'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '359370', 'page': 1, 'source': 'Wikipedia', 'title': 'Supervised learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'page': 1, 'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'source': 'Wikipedia', 'id': '6360', 'page': 1, 'title': 'Artificial intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'title': 'Artificial neural network', 'id': '44742', 'source': 'Wikipedia', 'page': 1}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"what is the difference between transformers and vision transformers?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"S_PXFMcJxuyO","outputId":"d649562a-213c-4242-e51c-a41816ae5482","executionInfo":{"status":"ok","timestamp":1752497313096,"user_tz":-330,"elapsed":468,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'page': 7, 'title': 'vision_transformer.pdf', 'id': '5f2a682d-4ec5-4dd4-9a64-bc314b90bc67', 'source': './rag_docs/vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers outperform ResNets in terms of compute efficiency and suggesting potential for further scaling efforts. Additionally, it discusses the performance of hybrid models in comparison to pure Vision Transformers.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plu"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf', 'page': 0, 'id': '4cc53577-e83f-4046-84b1-b132bd16b715'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the introduction of the Vision Transformer (ViT) architecture, which applies a standard Transformer model directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks with fewer computational resources when pre-trained on large datasets.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage "},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': 'af27625e-37c9-434c-a1d6-549c4e308ab7', 'page': 2, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁ"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': 'e9cf138e-adc2-42ea-8d24-2ae76a5d032a', 'page': 1, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the performance of the Vision Transformer (ViT) in image classification tasks, highlighting its ability to achieve state-of-the-art results when pre-trained on large datasets. It contrasts the inductive biases of convolutional neural networks (CNNs) with the advantages of large-scale training for ViT, demonstrating its effectiveness on various benchmarks, including ImageNet and CIFAR-100. Additionally, it references related work in the field of self-attention and image processing.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'title': 'vision_transformer.pdf', 'id': '59947efc-8e64-4097-94b9-8ddd9d25dc65', 'page': 7, 'source': './rag_docs/vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the behavior of attention mechanisms in the Vision Transformer (ViT), highlighting how attention distances vary across layers and the implications of localized attention in hybrid models that incorporate convolutional networks. It also discusses the relationship between attention distance and network depth, indicating that deeper layers attend to semantically relevant regions in images.\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6).\n4.6\nSELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excell"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"markdown","source":["## Build the RAG Pipeline"],"metadata":{"id":"gQFWv7YUyVII"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"],"metadata":{"id":"PHOrfGXKyVIJ","executionInfo":{"status":"ok","timestamp":1752497313107,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","qa_rag_chain = (\n","    {\n","        \"context\": (similarity_retriever\n","                      |\n","                    format_docs),\n","        \"question\": RunnablePassthrough()\n","    }\n","      |\n","    rag_prompt_template\n","      |\n","    chatgpt\n",")"],"metadata":{"id":"KmWeCB4yyVIJ","executionInfo":{"status":"ok","timestamp":1752497313115,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","query = \"What is machine learning?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"outputId":"3d0d47f8-3a13-407a-e059-07a7fad6420f","id":"xvj_eGIWyVIJ","executionInfo":{"status":"ok","timestamp":1752497319900,"user_tz":-330,"elapsed":6782,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Some common applications of machine learning include:\n\n- Spam filtering\n- Detection of network intruders or malicious insiders\n- Optical character recognition (OCR)\n- Search engines\n- Computer vision\n\nWithin machine learning, there are different types of learning approaches, such as supervised learning, where a function is inferred from labeled training data. In supervised learning, the system learns to produce correct results based on known outcomes from the training data.\n\nAdditionally, deep learning is a specialized area of machine learning that primarily utilizes neural networks. Deep learning involves multiple layers of processing, allowing the model to learn increasingly abstract representations of the data. This approach is particularly effective for complex tasks like speech recognition, image understanding, and handwriting recognition.\n\nOverall, machine learning enables computers to improve their performance on tasks through experience, making it a powerful tool in the field of artificial intelligence."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is a CNN?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":622},"id":"pXtezDlZzadt","outputId":"2a3bb904-d6ab-4dcb-8c6d-443f00750529","executionInfo":{"status":"ok","timestamp":1752497330649,"user_tz":-330,"elapsed":10746,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A CNN, or Convolutional Neural Network, is a specialized type of Artificial Neural Network (ANN) primarily used for image-driven pattern recognition tasks. CNNs are designed to process data with a grid-like topology, such as images, and they excel in recognizing patterns and features within visual data.\n\n### Key Characteristics of CNNs:\n\n1. **Architecture**:\n   - CNNs are composed of three main types of layers:\n     - **Convolutional Layers**: These layers apply convolution operations to the input, allowing the network to learn spatial hierarchies of features. Each neuron in a convolutional layer is connected to a small region of the input, which helps in detecting local patterns.\n     - **Pooling Layers**: These layers reduce the spatial dimensions of the input, helping to decrease the computational load and control overfitting by summarizing the features.\n     - **Fully-Connected Layers**: These layers connect every neuron in one layer to every neuron in the next layer, typically used at the end of the network to produce the final output.\n\n2. **Dimensionality**:\n   - The neurons in CNNs are organized into three dimensions: height, width, and depth. This structure allows CNNs to effectively process and analyze image data, where height and width correspond to the spatial dimensions of the image, and depth corresponds to the number of channels (e.g., RGB for color images).\n\n3. **Feature Extraction**:\n   - CNNs are particularly effective at extracting features from images through the stacking of multiple convolutional layers, which allows for the selection of increasingly complex features as the data passes through the network.\n\n4. **Learning Process**:\n   - Similar to traditional ANNs, CNNs learn by adjusting the weights of connections based on the input data and the associated output. They utilize techniques such as backpropagation to optimize their performance.\n\n5. **Applications**:\n   - CNNs are widely used in various applications, including image classification, object detection, and facial recognition, due to their ability to automatically learn and encode image-specific features.\n\nIn summary, CNNs represent a powerful and efficient architecture for processing visual data, leveraging their unique layer structure to excel in tasks related to image recognition and analysis."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How is a resnet better than a CNN?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"Fo92-ZmIELPF","outputId":"34365313-dd49-44d7-cd70-630459b762f3","executionInfo":{"status":"ok","timestamp":1752497340486,"user_tz":-330,"elapsed":9833,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A ResNet (Residual Network) is considered better than a traditional CNN (Convolutional Neural Network) for several reasons, primarily due to its architectural innovations that address optimization challenges associated with deeper networks. Here are the key advantages of ResNets over standard CNNs:\n\n1. **Residual Learning Framework**: ResNets introduce the concept of residual learning, where the network learns to predict the residual (the difference) between the desired output and the input. This is formalized as \\( F(x) + x \\), where \\( F(x) \\) is the residual mapping. This approach simplifies the learning process, making it easier for the network to optimize.\n\n2. **Shortcut Connections**: ResNets utilize shortcut connections that skip one or more layers. These connections allow gradients to flow more easily during backpropagation, mitigating issues like vanishing gradients that often occur in very deep networks. As a result, ResNets can be trained effectively even with a significantly increased number of layers.\n\n3. **Improved Training and Validation Performance**: Empirical evidence shows that ResNets achieve lower training and validation errors compared to their plain counterparts. For instance, in experiments on the ImageNet dataset, a 34-layer ResNet outperformed a 34-layer plain network by a notable margin (25.03% vs. 28.54% top-1 error). This indicates that ResNets can generalize better and achieve higher accuracy.\n\n4. **Ability to Train Deeper Networks**: Traditional CNNs often suffer from degradation problems as the depth increases, leading to higher training errors. In contrast, ResNets can maintain or even improve performance as they become deeper, as demonstrated by the successful training of networks with over 100 layers, including the 152-layer ResNet, which achieved state-of-the-art results on various benchmarks.\n\n5. **Lower Computational Complexity**: Despite their increased depth, ResNets can maintain lower computational complexity compared to other architectures like VGG networks. For example, a 152-layer ResNet has lower FLOPs (floating point operations) than VGG-16, making it more efficient.\n\n6. **Generalization Across Tasks**: ResNets have shown strong generalization performance across various recognition tasks, not just in image classification but also in object detection and segmentation, leading to significant performance gains in competitions like ILSVRC and COCO.\n\nIn summary, ResNets improve upon traditional CNNs by addressing optimization difficulties, enabling the training of deeper networks, and achieving better performance with lower computational costs."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is NLP and its relation to linguistics?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"id":"J5IQoBc0zlAr","outputId":"6028f1a2-1aea-4451-b359-0e9f987ef862","executionInfo":{"status":"ok","timestamp":1752497343766,"user_tz":-330,"elapsed":3271,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Natural Language Processing (NLP) is a field within Artificial Intelligence that focuses on enabling computers to automatically understand and generate human languages. The term \"Natural Language\" specifically refers to human languages, distinguishing them from programming languages. The overarching goal of NLP is to facilitate seamless interaction between humans and machines through language.\n\nNLP is closely related to linguistics, which is the scientific study of language and its structure. Linguistics provides the foundational theories and frameworks that inform the development of NLP technologies. By leveraging insights from linguistics, NLP aims to improve the accuracy and effectiveness of language understanding and generation tasks performed by computers. This relationship underscores the importance of linguistic principles in the design and implementation of NLP systems."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is the difference between AI, ML and DL?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"id":"AzeZuG1hzvGy","outputId":"f939fb6f-d096-477a-93aa-72342eddceb4","executionInfo":{"status":"ok","timestamp":1752497352671,"user_tz":-330,"elapsed":8902,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications that enable machines to perform tasks that typically require human intelligence, such as learning, reasoning, and problem-solving.\n- **Origin**: The term \"Artificial Intelligence\" was coined by John McCarthy in 1955.\n- **Functionality**: AI systems interpret external data, learn from it, and adapt to achieve specific goals. As technology evolves, tasks once considered to require intelligence (like optical character recognition) may no longer be classified as AI.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that focuses on the development of algorithms that allow computers to learn from and make predictions based on data without being explicitly programmed for each task.\n- **Functionality**: ML algorithms build models from sample inputs and can make predictions or decisions based on data. It is particularly useful in scenarios where traditional programming is impractical, such as spam filtering and network intrusion detection.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of machine learning that primarily uses neural networks with multiple layers (known as deep neural networks) to analyze various forms of data.\n- **Structure**: Deep learning models often include at least one hidden layer between the input and output layers, allowing for the processing of more abstract representations of data as it passes through these layers.\n- **Applications**: DL is particularly effective for complex tasks such as speech recognition, image classification, and understanding handwriting, which are challenging for traditional algorithms.\n\nIn summary, AI is the overarching field that includes both ML and DL. ML is a method within AI that enables learning from data, while DL is a more advanced technique within ML that utilizes deep neural networks for processing complex data."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is the difference between transformers and vision transformers?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"id":"cWihDiL3zPzY","outputId":"0f0d8fc5-ebd8-4da9-cd5b-d4adce429aa8","executionInfo":{"status":"ok","timestamp":1752497362199,"user_tz":-330,"elapsed":9522,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between transformers and vision transformers (ViTs) primarily lies in their application and the way they process input data.\n\n1. **Input Representation**:\n   - **Transformers**: In traditional transformers, the input consists of sequences of tokens, typically used in natural language processing (NLP). Each token represents a word or a sub-word in a sentence.\n   - **Vision Transformers (ViTs)**: ViTs adapt the transformer architecture for image classification tasks by treating image patches as tokens. An image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence of vectors. This sequence is processed similarly to how words are processed in NLP.\n\n2. **Architecture**:\n   - **Transformers**: The standard transformer architecture includes layers of multi-headed self-attention and feed-forward neural networks, designed to capture relationships between tokens in a sequence.\n   - **Vision Transformers**: ViTs maintain the core transformer architecture but modify the input to accommodate 2D image data. They utilize position embeddings to retain spatial information about the patches, allowing the model to understand the layout of the image.\n\n3. **Performance and Efficiency**:\n   - **Transformers**: In NLP, transformers have become the standard due to their ability to scale and perform well on large datasets, often requiring substantial computational resources.\n   - **Vision Transformers**: ViTs have shown competitive performance in image classification tasks, often outperforming traditional convolutional neural networks (CNNs) like ResNets in terms of compute efficiency. They can achieve high accuracy with fewer computational resources when pre-trained on large datasets, such as JFT-300M.\n\n4. **Inductive Bias**:\n   - **Transformers**: Traditional transformers do not have inherent inductive biases related to the structure of language, relying on large amounts of data for training.\n   - **Vision Transformers**: ViTs lack some of the inductive biases present in CNNs, such as translation equivariance and locality, which can affect their performance on smaller datasets. However, when trained on sufficiently large datasets, ViTs can outperform CNNs.\n\nIn summary, while both transformers and vision transformers share a common architectural foundation, they differ significantly in their input data types, processing methods, and performance characteristics in their respective domains."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How is self-attention important in transformers?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405},"id":"k1lqzejlEvsj","outputId":"533c5632-e87d-4093-ca6d-3d5c98b9d8fb","executionInfo":{"status":"ok","timestamp":1752497372852,"user_tz":-330,"elapsed":10647,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Self-attention is crucial in transformers, particularly in the Vision Transformer (ViT), for several reasons:\n\n1. **Integration of Information**: Self-attention allows the ViT to integrate information across the entire image, even in the lowest layers. This capability enables the model to attend to various parts of the image simultaneously, which is essential for understanding complex visual patterns and relationships.\n\n2. **Attention Distance**: The concept of \"attention distance\" is significant in understanding how self-attention operates within the ViT. It refers to the average distance in image space across which information is integrated based on the attention weights. This attention distance is analogous to the receptive field size in convolutional neural networks (CNNs). The findings indicate that some attention heads can attend to most of the image from the lowest layers, demonstrating the model's ability to utilize global information effectively.\n\n3. **Layer-wise Behavior**: The behavior of attention mechanisms varies across layers. In the lower layers, attention distances are consistently small, indicating highly localized attention. As the network depth increases, the attention distance grows, allowing deeper layers to focus on semantically relevant regions of the image. This progression enhances the model's ability to capture more abstract and meaningful features as it processes the input.\n\n4. **Comparison with Hybrid Models**: The attention mechanism in ViT is less pronounced in hybrid models that incorporate convolutional networks, suggesting that self-attention may serve a similar function to early convolutional layers in CNNs. This highlights the unique role of self-attention in enabling the ViT to achieve state-of-the-art results in image classification tasks, particularly when trained on large datasets.\n\nIn summary, self-attention in transformers, especially in the context of the Vision Transformer, is vital for enabling global information integration, adapting attention distances across layers, and enhancing the model's performance in visual tasks."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How does a resnet work?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":724},"id":"7Zf_BjmlFBcb","outputId":"b685d7aa-3c96-4a60-dc2e-5b9e60924c24","executionInfo":{"status":"ok","timestamp":1752497383698,"user_tz":-330,"elapsed":10854,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A ResNet, or Residual Network, operates on the principle of residual learning to address the challenges associated with training deep neural networks. Here’s a detailed explanation of how it works:\n\n### Key Concepts of ResNet\n\n1. **Residual Learning**:\n   - Instead of learning the desired underlying mapping \\( H(x) \\) directly, ResNets learn a residual mapping \\( F(x) = H(x) - x \\). This means that the network is designed to learn the difference between the desired output and the input, which is often easier than learning the output directly.\n\n2. **Shortcut Connections**:\n   - ResNets utilize shortcut connections that skip one or more layers. These connections perform identity mapping, allowing the input \\( x \\) to be added directly to the output of the stacked layers. This can be mathematically represented as \\( F(x) + x \\).\n   - The use of these shortcuts helps to mitigate the degradation problem, where deeper networks tend to perform worse than their shallower counterparts due to optimization difficulties.\n\n3. **Optimization Benefits**:\n   - The architecture allows for easier optimization of deeper networks. Empirical evidence shows that ResNets can achieve lower training errors and better generalization as the depth of the network increases, unlike plain networks which suffer from higher training errors with increased depth.\n\n### Architectural Details\n\n- ResNets can be constructed with various depths, such as 18, 34, 50, 101, and 152 layers. Each layer typically consists of convolutional operations followed by batch normalization and ReLU activation functions.\n- The architecture includes bottleneck blocks, especially in deeper networks (like the 50, 101, and 152-layer ResNets), which help maintain lower computational complexity while achieving significant accuracy gains.\n\n### Performance\n\n- ResNets have demonstrated superior performance on benchmark datasets like ImageNet and CIFAR-10. For instance, a 152-layer ResNet achieved a top-5 validation error of 4.49% on ImageNet, outperforming previous models and winning the ILSVRC 2015 competition.\n- The architecture allows for training extremely deep networks (over 100 layers) effectively, which was previously challenging with traditional deep learning architectures.\n\n### Conclusion\n\nIn summary, ResNets leverage the concept of residual learning and shortcut connections to facilitate the training of very deep neural networks, overcoming the optimization challenges that typically arise with increased depth. This innovative approach has led to significant improvements in accuracy and performance across various image recognition tasks."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is LangGraph?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"UbaUpVXKz8IK","outputId":"6a531788-1f82-431b-f1b0-4f29569d2c85","executionInfo":{"status":"ok","timestamp":1752497386090,"user_tz":-330,"elapsed":2389,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I don't know."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is an Agentic AI System?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":64},"id":"FmxNo6B50AdP","outputId":"c168c469-2fca-4d86-c2dd-7a6e21acb605","executionInfo":{"status":"ok","timestamp":1752497388505,"user_tz":-330,"elapsed":2417,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is LangChain?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"Ttz53mEy0J_D","outputId":"f3c64383-32fa-4ec7-c19d-68e15c6cfa51","executionInfo":{"status":"ok","timestamp":1752497389490,"user_tz":-330,"elapsed":979,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I don't know."},"metadata":{}}]},{"cell_type":"markdown","source":["# Build a RAG System with Sources"],"metadata":{"id":"-HkUAnWFH8LA"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"],"metadata":{"id":"cd9CWH6hH8LB","executionInfo":{"status":"ok","timestamp":1752497389496,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.runnables import RunnableLambda\n","from operator import itemgetter\n","\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","src_rag_response_chain = (\n","    {\n","        \"context\": (itemgetter('context')\n","                        |\n","                    RunnableLambda(format_docs)),\n","        \"question\": itemgetter(\"question\")\n","    }\n","        |\n","    rag_prompt_template\n","        |\n","    chatgpt\n","        |\n","    StrOutputParser()\n",")\n","\n","rag_chain_w_sources = (\n","    {\n","        \"context\": similarity_retriever,\n","        \"question\": RunnablePassthrough()\n","    }\n","        |\n","    RunnablePassthrough.assign(response=src_rag_response_chain)\n",")"],"metadata":{"id":"m6wFtKQqXNHE","executionInfo":{"status":"ok","timestamp":1752497389504,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["query = \"What is machine learning?\"\n","result = rag_chain_w_sources.invoke(query)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYfU-VdjYf1u","outputId":"5a0deff3-f6f6-4746-dbee-772fd02be5a6","executionInfo":{"status":"ok","timestamp":1752497394944,"user_tz":-330,"elapsed":5436,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(id='f41be5b7-f487-4157-be22-4a9f1e7abb2a', metadata={'page': 1, 'title': 'Machine learning', 'source': 'Wikipedia', 'id': '564928'}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n","  Document(id='9d1a4805-deb9-43b1-9478-a8e0207cca93', metadata={'source': 'Wikipedia', 'title': 'Supervised learning', 'page': 1, 'id': '359370'}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n","  Document(id='8f8516b8-b50d-421b-9162-1220e25c24a4', metadata={'id': '663523', 'page': 1, 'title': 'Deep learning', 'source': 'Wikipedia'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n","  Document(id='db1c7e4d-7e1e-4078-9ebc-77cbd15d5cbe', metadata={'page': 1, 'title': 'Artificial intelligence', 'source': 'Wikipedia', 'id': '6360'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n","  Document(id='0dabfe4f-202c-4e23-b40f-391b7f1a3be6', metadata={'title': 'Artificial neural network', 'page': 1, 'id': '44742', 'source': 'Wikipedia'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')],\n"," 'question': 'What is machine learning?',\n"," 'response': 'Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\\n\\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Some common applications of machine learning include:\\n\\n- Spam filtering\\n- Detection of network intruders or malicious insiders\\n- Optical character recognition (OCR)\\n- Search engines\\n- Computer vision\\n\\nWithin machine learning, there are different approaches, such as supervised learning, where a function is inferred from labeled training data. In this case, the system learns to produce correct results based on known outcomes from the training data.\\n\\nAdditionally, deep learning is a specialized area of machine learning that primarily utilizes neural networks. Deep learning involves multiple layers of processing, allowing the model to learn increasingly abstract representations of the data. This approach is particularly effective for complex tasks like speech recognition, image understanding, and handwriting recognition.\\n\\nOverall, machine learning enables computers to improve their performance on tasks through experience, making it a powerful tool in the field of artificial intelligence.'}"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_results(result_obj):\n","    print('Query:')\n","    display(Markdown(result_obj['question']))\n","    print()\n","    print('Response:')\n","    display(Markdown(result_obj['response']))\n","    print('='*50)\n","    print('Sources:')\n","    for source in result_obj['context']:\n","        print('Metadata:', source.metadata)\n","        print('Content Brief:')\n","        display(Markdown(source.page_content))\n","        print()\n"],"metadata":{"id":"2wWdkU4oa1BD","executionInfo":{"status":"ok","timestamp":1752497394947,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["query = \"What is machine learning?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8NTfGcChbFuj","outputId":"5161ca78-a1d7-4e4a-a124-01901d04ef69","executionInfo":{"status":"ok","timestamp":1752497400696,"user_tz":-330,"elapsed":5748,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is machine learning?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Some common applications of machine learning include:\n\n- Spam filtering\n- Detection of network intruders or malicious insiders\n- Optical character recognition (OCR)\n- Search engines\n- Computer vision\n\nWithin machine learning, there are different approaches, such as supervised learning, where a function is inferred from labeled training data. In this case, the system learns to produce correct results based on known outcomes, typically using vectors for training data and results.\n\nAdditionally, deep learning is a specialized area of machine learning that utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). Deep learning is effective for complex tasks like speech recognition, image understanding, and handwriting recognition, which are challenging for computers but relatively easy for humans. \n\nOverall, machine learning represents a significant advancement in the ability of computers to process information and make decisions autonomously, evolving from traditional programming methods."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'id': '564928', 'source': 'Wikipedia', 'page': 1, 'title': 'Machine learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'source': 'Wikipedia', 'page': 1, 'title': 'Supervised learning', 'id': '359370'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'title': 'Deep learning', 'source': 'Wikipedia', 'id': '663523', 'page': 1}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'page': 1, 'id': '6360', 'title': 'Artificial intelligence', 'source': 'Wikipedia'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'page': 1, 'id': '44742', 'title': 'Artificial neural network', 'source': 'Wikipedia'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is the difference between AI, ML and DL?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jhWbcmodbYfy","outputId":"4e024ae5-1139-49da-8afb-970e2efa9bd8","executionInfo":{"status":"ok","timestamp":1752497409188,"user_tz":-330,"elapsed":8486,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is the difference between AI, ML and DL?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications aimed at making machines \"smart.\"\n- **Scope**: AI includes various subfields, one of which is machine learning. It focuses on systems that can interpret external data, learn from it, and adapt to achieve specific goals.\n- **Examples**: AI applications can range from simple tasks like optical character recognition to more complex problem-solving scenarios.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that enables computers to learn from data without being explicitly programmed. It involves the study and construction of algorithms that can make predictions or decisions based on input data.\n- **Functionality**: ML algorithms build models from sample inputs and can operate in various learning modes, including supervised, unsupervised, and semi-supervised learning.\n- **Examples**: Common applications of ML include spam filtering, network intrusion detection, and computer vision tasks.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of machine learning that primarily uses neural networks with multiple layers (deep neural networks) to process data. It is particularly effective for tasks that require understanding complex patterns in large datasets.\n- **Architecture**: Deep learning models often have at least one hidden layer between the input and output layers, allowing them to learn increasingly abstract representations of the data.\n- **Examples**: DL is commonly used in applications such as speech recognition, image classification, and natural language processing.\n\nIn summary, AI is the overarching field that includes both ML and DL, with ML being a method of achieving AI through data-driven learning, and DL being a more advanced technique within ML that utilizes deep neural networks for complex tasks."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'page': 1, 'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'source': 'Wikipedia', 'page': 1, 'title': 'Machine learning', 'id': '564928'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'source': 'Wikipedia', 'id': '6360', 'title': 'Artificial intelligence', 'page': 1}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '69ccb3c9-9e7c-4dcb-8781-e32a8611be33', 'page': 3, 'title': 'cnn_paper.pdf', 'source': './rag_docs/cnn_paper.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the architectural differences of Convolutional Neural Networks (CNNs) compared to traditional Artificial Neural Networks (ANNs), emphasizing the three-dimensional organization of neurons and the specific types of layers that comprise CNNs, including convolutional, pooling, and fully-connected layers. It also outlines the basic functionality of these layers in processing image data for tasks such as classification.\n4\nKeiron O’Shea et al.\nOne of the key differences is that the neurons that the layers within the CNN\nare comprised of neurons organised into three dimensions, the spatial dimen-\nsionality of the input (height and the width) and the depth. The depth does not\nrefer to the total number of layers within the ANN, but the third dimension of a\nactivation volume. Unlike standard ANNS, the neurons within any given layer\nwill only connect to a small region of the layer preceding it.\nIn practice this would mean that for the example given earlier, the input ’vol-\nume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), lead-\ning to a ﬁnal output layer comprised of a dimensionality of 1 × 1 × n (where\nn represents the possible number of classes) as we would have condensed the\nfull input dimensionality into a smaller volume of class scores ﬁled across the\ndepth dimension.\n2.1\nOverall architecture\nCNNs are comprised of three types of layers. These are convolutional layers,\npooling layers and fully-connected layers. When these layers are stacked, a\nCNN architecture has been formed. A simpliﬁed CNN architecture for MNIST\nclassiﬁcation is illustrated in Figure 2.\ninput\n0\n9\nconvolution\n w/ReLu\npooling\noutput\nfully-connected\nw/ ReLu\nfully-connected\n...\nFig. 2: An simple CNN architecture, comprised of just ﬁve layers\nThe basic functionality of the example CNN above can be broken down into\nfour key areas.\n1. As found in other forms of ANN, the input layer will hold the pixel values\nof the image.\n2. The convolutional layer will determine the output of neurons of which are\nconnected to local regions of the input through the calculation of the scalar\nproduct between their weights and the region connected to the input vol-\nume. The rectiﬁed linear unit (commonly shortened to ReLu) aims to apply"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'page': 1, 'title': 'Loop AI Labs', 'source': 'Wikipedia', 'id': '669662'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Loop AI Labs is an AI and cognitive computing company that focuses on language understanding technology. The company was founded in San Francisco in 2012 by Italian entrepreneur Gianmauro Calafiore, who sold his company Gsmbox to in 2004 and then relocated from Italy to San Francisco. Wanting to start an artificial intelligence company, he recruited two veterans of the project, the largest government-funded AI project in history, who had worked on the project at and Stanford University's . The original company name, \"Soshoma\", was changed to Loop AI Labs in 2015 after the company decided to change its focus from consumer-oriented to enterprise. Loop AI Labs is headquartered in San Francisco, California, with offices in New York, Milan, and Singapore. The company is privately funded. On May 4, 2017, Loop AI Labs entered into a deal with , a leading European provider of mobile messaging and solutions, to bring their cognitive computing technology to LINK's business clients, which cover 234 million people across Europe."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is the difference between transformers and vision transformers?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"iKggz7qfYkh1","outputId":"be285e77-80a2-4226-90c3-e79052dbcd94","executionInfo":{"status":"ok","timestamp":1752497418668,"user_tz":-330,"elapsed":9477,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is the difference between transformers and vision transformers?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between transformers and vision transformers (ViTs) primarily lies in their application and the way they process input data.\n\n1. **Input Representation**:\n   - **Transformers**: In traditional transformers, the input is typically a sequence of tokens, such as words in natural language processing (NLP). Each token is represented as an embedding, and the model processes these embeddings using self-attention mechanisms.\n   - **Vision Transformers (ViTs)**: ViTs adapt this architecture for image classification tasks by treating image patches as tokens. An image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence of vectors. This sequence is fed into the transformer model, similar to how word embeddings are processed in NLP.\n\n2. **Architecture**:\n   - **Transformers**: The standard transformer architecture consists of layers of multi-headed self-attention and feed-forward neural networks, designed to capture relationships between tokens in a sequence.\n   - **Vision Transformers**: ViTs maintain the same basic architecture as standard transformers but are specifically designed to handle the spatial structure of images. They incorporate position embeddings to retain spatial information about the patches, allowing the model to understand the layout of the image.\n\n3. **Performance and Efficiency**:\n   - **Transformers**: In NLP, transformers have become the de-facto standard due to their ability to scale and perform well on large datasets.\n   - **Vision Transformers**: ViTs have shown competitive performance in image classification tasks, often outperforming traditional convolutional neural networks (CNNs) like ResNets in terms of compute efficiency when pre-trained on large datasets. They require significantly less computational resources to achieve similar or better performance compared to CNNs.\n\n4. **Inductive Bias**:\n   - **Transformers**: Traditional transformers do not have inherent inductive biases related to the structure of the data they process.\n   - **Vision Transformers**: ViTs lack some of the inductive biases present in CNNs, such as translation equivariance and locality, which can affect their performance on smaller datasets. However, when trained on large datasets, ViTs can leverage their architecture to achieve high accuracy.\n\nIn summary, while both transformers and vision transformers share a common architectural foundation, they differ in their input data representation, application domain, and performance characteristics, particularly in how they handle image data compared to sequential text data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'source': './rag_docs/vision_transformer.pdf', 'id': '5f2a682d-4ec5-4dd4-9a64-bc314b90bc67', 'title': 'vision_transformer.pdf', 'page': 7}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers outperform ResNets in terms of compute efficiency and suggesting potential for further scaling efforts. Additionally, it discusses the performance of hybrid models in comparison to pure Vision Transformers.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\n4.5\nINSPECTING VISION TRANSFORMER\nInput\nAttention\nFigure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails.\nTo begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The ﬁrst layer of\nthe Vision Transformer linearly projects the ﬂattened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding ﬁlters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the ﬁne structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4).\nSelf-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speciﬁcally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '4cc53577-e83f-4046-84b1-b132bd16b715', 'source': './rag_docs/vision_transformer.pdf', 'page': 0, 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the introduction of the Vision Transformer (ViT) architecture, which applies a standard Transformer model directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks with fewer computational resources when pre-trained on large datasets.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1\n1\nINTRODUCTION\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020).\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classiﬁcation in supervised fashion.\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n1Fine-tuning\ncode\nand\npre-trained\nmodels\nare\navailable\nat\nhttps://github.com/\ngoogle-research/vision_transformer\n1\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'page': 2, 'id': 'af27625e-37c9-434c-a1d6-549c4e308ab7', 'title': 'vision_transformer.pdf', 'source': './rag_docs/vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3\nMETHOD\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\ntheir efﬁcient implementations – can be used almost out of the box.\n3.1\nVISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder (z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\ntached to z0\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at ﬁne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n3"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'title': 'vision_transformer.pdf', 'source': './rag_docs/vision_transformer.pdf', 'page': 1, 'id': 'e9cf138e-adc2-42ea-8d24-2ae76a5d032a'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the performance of the Vision Transformer (ViT) in image classification tasks, highlighting its ability to achieve state-of-the-art results when pre-trained on large datasets. It contrasts the inductive biases of convolutional neural networks (CNNs) with the advantages of large-scale training for ViT, demonstrating its effectiveness on various benchmarks, including ImageNet and CIFAR-100. Additionally, it references related work in the field of self-attention and image processing.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks.\n2\nRELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\nNaive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nefﬁciently on hardware accelerators.\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'page': 7, 'title': 'vision_transformer.pdf', 'id': '59947efc-8e64-4097-94b9-8ddd9d25dc65', 'source': './rag_docs/vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the behavior of attention mechanisms in the Vision Transformer (ViT), highlighting how attention distances vary across layers and the implications of localized attention in hybrid models that incorporate convolutional networks. It also discusses the relationship between attention distance and network depth, indicating that deeper layers attend to semantically relevant regions in images.\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6).\n4.6\nSELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\n8"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is an Agentic AI System?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":972},"id":"fCCvcwzEbqQq","outputId":"68dc560c-6586-4508-f663-1f1f664f64d5","executionInfo":{"status":"ok","timestamp":1752497422129,"user_tz":-330,"elapsed":3459,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is an Agentic AI System?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'source': 'Wikipedia', 'page': 1, 'title': 'Artificial intelligence', 'id': '6360'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '674015', 'page': 1, 'title': 'A.I. Artificial Intelligence', 'source': 'Wikipedia'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A.I. Artificial Intelligence, or A.I., is a 2001 American science fiction drama movie directed by Steven Spielberg. The screenplay was by Spielberg based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. The movie was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. It is set in a futuristic post-climate change society. \"A.I.\" tells the story of David (Osment), a childlike android uniquely programmed with the ability to love."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'title': 'Swarm intelligence', 'id': '112634', 'page': 1, 'source': 'Wikipedia'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Swarm Intelligence is a field of Computer science. It is a form of Artificial intelligence. Some animals, mostly insects like ants, or bees form large colonies. These colonies are made of many animals that communicate with each other. Each animal is relatively simple, but by working together with other animals it is able to solve complex tasks. Swarm intelligence wants to obtain similar behaviour than that observed with these animals. Instead of the animals, so called \"agents\" are used."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'title': 'Shakey the robot', 'page': 1, 'source': 'Wikipedia', 'id': '692745'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Shakey the Robot was the first general purpose mobile AI robot. The project combined research in robotics, computer vision, and natural language processing. Because of this, it was the first project that melded logical reasoning and physical action. Shakey was developed at the Artificial Intelligence Center of Stanford Research Institute (now called SRI International) by Nils John Nilsson from 1966 to 1972."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'page': 1, 'title': 'Robot lawyer', 'source': 'Wikipedia', 'id': '564218'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A robot lawyer is an artificial intelligence (AI) computer program. It is designed to ask the same questions as a real lawyer about certain legal issues. Robot lawyers are being used in many countries around the world including the United States, the United Kingdom, and Holland."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"markdown","source":["# Build a RAG System with Source Citations Agentic Pipeline"],"metadata":{"id":"25Si_mSAc9HY"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n","rag_prompt_template.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3cKwTUIc9HZ","outputId":"e9daf5c2-7cd7-4d3a-9b16-cc7c21176577","executionInfo":{"status":"ok","timestamp":1752497422152,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n","\n","                Context:\n","                \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n","\n","                Answer:\n","            \n"]}]},{"cell_type":"code","source":["citations_prompt = \"\"\"You are an assistant who is an expert in analyzing answers to questions\n","                      and finding out referenced citations from context articles.\n","\n","                      Given the following question, context and generated answer,\n","                      analyze the generated answer and quote citations from context articles\n","                      that can be used to justify the generated answer.\n","\n","                      Question:\n","                      {question}\n","\n","                      Context Articles:\n","                      {context}\n","\n","                      Answer:\n","                      {answer}\n","                  \"\"\"\n","\n","cite_prompt_template = ChatPromptTemplate.from_template(citations_prompt)\n","cite_prompt_template.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fbhOllRprRx","outputId":"4f660a47-1aa5-414b-e36b-e7a358644ff8","executionInfo":{"status":"ok","timestamp":1752497422168,"user_tz":-330,"elapsed":13,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","You are an assistant who is an expert in analyzing answers to questions\n","                      and finding out referenced citations from context articles.\n","\n","                      Given the following question, context and generated answer,\n","                      analyze the generated answer and quote citations from context articles\n","                      that can be used to justify the generated answer.\n","\n","                      Question:\n","                      \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n","\n","                      Context Articles:\n","                      \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n","\n","                      Answer:\n","                      \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n","                  \n"]}]},{"cell_type":"code","source":["from pydantic import BaseModel, Field\n","from typing import List\n","\n","class Citation(BaseModel):\n","    id: str = Field(description=\"\"\"The string ID of a SPECIFIC context article\n","                                   which justifies the answer.\"\"\")\n","    source: str = Field(description=\"\"\"The source of the SPECIFIC context article\n","                                       which justifies the answer.\"\"\")\n","    title: str = Field(description=\"\"\"The title of the SPECIFIC context article\n","                                      which justifies the answer.\"\"\")\n","    page: int = Field(description=\"\"\"The page number of the SPECIFIC context article\n","                                     which justifies the answer.\"\"\")\n","    quotes: str = Field(description=\"\"\"The VERBATIM sentences from the SPECIFIC context article\n","                                      that are used to generate the answer.\n","                                      Should be exact sentences from context article without missing words.\"\"\")\n","\n","\n","class QuotedCitations(BaseModel):\n","    \"\"\"Quote citations from given context articles\n","       that can be used to justify the generated answer. Can be multiple articles.\"\"\"\n","    citations: List[Citation] = Field(description=\"\"\"Citations (can be multiple) from the given\n","                                                     context articles that justify the answer.\"\"\")"],"metadata":{"id":"n-ehwnM4dyGV","executionInfo":{"status":"ok","timestamp":1752497422174,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.runnables import RunnableLambda\n","from operator import itemgetter\n","\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","structured_chatgpt = chatgpt.with_structured_output(QuotedCitations)\n","\n","\n","def format_docs_with_metadata(docs: List[Document]) -> str:\n","    formatted_docs = [\n","        f\"\"\"Context Article ID: {doc.metadata['id']}\n","            Context Article Source: {doc.metadata['source']}\n","            Context Article Title: {doc.metadata['title']}\n","            Context Article Page: {doc.metadata['page']}\n","            Context Article Details: {doc.page_content}\n","         \"\"\"\n","            for i, doc in enumerate(docs)\n","    ]\n","    return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n","\n","rag_response_chain = (\n","    {\n","        \"context\": (itemgetter('context')\n","                        |\n","                    RunnableLambda(format_docs_with_metadata)),\n","        \"question\": itemgetter(\"question\")\n","    }\n","        |\n","    rag_prompt_template\n","        |\n","    chatgpt\n","        |\n","    StrOutputParser()\n",")\n","\n","cite_response_chain = (\n","    {\n","        \"context\": itemgetter('context'),\n","        \"question\": itemgetter(\"question\"),\n","        \"answer\": itemgetter(\"answer\")\n","    }\n","        |\n","    cite_prompt_template\n","        |\n","    structured_chatgpt\n",")\n","\n","rag_chain_w_citations = (\n","    {\n","        \"context\": similarity_retriever,\n","        \"question\": RunnablePassthrough()\n","    }\n","        |\n","    RunnablePassthrough.assign(answer=rag_response_chain)\n","        |\n","    RunnablePassthrough.assign(citations=cite_response_chain)\n",")"],"metadata":{"id":"haxp_9LmjVp1","executionInfo":{"status":"ok","timestamp":1752497422204,"user_tz":-330,"elapsed":26,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["query = \"What is machine learning\"\n","result = rag_chain_w_citations.invoke(query)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"10af3de2-fd53-43bb-ab69-6d61d9cc643d","id":"Qt12nMvTjVp2","executionInfo":{"status":"ok","timestamp":1752497430073,"user_tz":-330,"elapsed":7865,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(id='f41be5b7-f487-4157-be22-4a9f1e7abb2a', metadata={'id': '564928', 'title': 'Machine learning', 'source': 'Wikipedia', 'page': 1}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n","  Document(id='9d1a4805-deb9-43b1-9478-a8e0207cca93', metadata={'id': '359370', 'page': 1, 'title': 'Supervised learning', 'source': 'Wikipedia'}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n","  Document(id='8f8516b8-b50d-421b-9162-1220e25c24a4', metadata={'id': '663523', 'page': 1, 'title': 'Deep learning', 'source': 'Wikipedia'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n","  Document(id='db1c7e4d-7e1e-4078-9ebc-77cbd15d5cbe', metadata={'id': '6360', 'page': 1, 'title': 'Artificial intelligence', 'source': 'Wikipedia'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n","  Document(id='0dabfe4f-202c-4e23-b40f-391b7f1a3be6', metadata={'title': 'Artificial neural network', 'page': 1, 'id': '44742', 'source': 'Wikipedia'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')],\n"," 'question': 'What is machine learning',\n"," 'answer': 'Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in the broader field of artificial intelligence (AI). \\n\\nMachine learning involves the study and construction of algorithms that can learn from and make predictions based on data. These algorithms follow programmed instructions but can also make decisions or predictions based on the data they process. The process typically involves building a model from sample inputs, which allows the system to operate in scenarios where designing and programming explicit algorithms is not feasible.\\n\\nSome common applications of machine learning include:\\n- Spam filtering\\n- Detection of network intruders or malicious insiders\\n- Optical character recognition (OCR)\\n- Search engines\\n- Computer vision\\n\\nOverall, machine learning enables systems to improve their performance on tasks over time as they are exposed to more data, making it a powerful tool in various domains.',\n"," 'citations': QuotedCitations(citations=[Citation(id='564928', source='Wikipedia', title='Machine learning', page=1, quotes='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.')])}"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["result['citations'].dict()['citations']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3fufsoAHiuD","outputId":"8ab0ffef-bc62-4812-fceb-8a8a799e2f3c","executionInfo":{"status":"ok","timestamp":1752497430097,"user_tz":-330,"elapsed":20,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":72,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-72-3486563740.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  result['citations'].dict()['citations']\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'id': '564928',\n","  'source': 'Wikipedia',\n","  'title': 'Machine learning',\n","  'page': 1,\n","  'quotes': 'Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'}]"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["import re\n","# used mostly for nice display formatting, ignore if not needed\n","def get_cited_context(result_obj):\n","    # Dictionary to hold separate citation information for each unique source and title combination\n","    source_with_citations = {}\n","\n","    def highlight_text(context, quote):\n","        # Normalize whitespace and remove unnecessary punctuation\n","        quote = re.sub(r'\\s+', ' ', quote).strip()\n","        context = re.sub(r'\\s+', ' ', context).strip()\n","\n","        # Split quote into phrases, being careful with punctuation\n","        phrases = [phrase.strip() for phrase in re.split(r'[.!?]', quote) if phrase.strip()]\n","\n","        highlighted_context = context\n","\n","        for phrase in phrases: # for each quoted phrase\n","\n","            # Create regex pattern to match cited phrases\n","            # Escape special regex characters, but preserve word boundaries\n","            escaped_phrase = re.escape(phrase)\n","            # Create regex pattern that allows for slight variations\n","            pattern = re.compile(r'\\b' + escaped_phrase + r'\\b', re.IGNORECASE)\n","\n","            # Replace all matched phrases with bolded version\n","            highlighted_context = pattern.sub(lambda m: f\"**{m.group(0)}**\", highlighted_context)\n","\n","        return highlighted_context\n","\n","    # Process the citation data\n","    for cite in result_obj['citations'].dict()['citations']:\n","        cite_id = cite['id']\n","        title = cite['title']\n","        source = cite['source']\n","        page = cite['page']\n","        quote = cite['quotes']\n","\n","        # Check if the (source, title) key exists, and initialize if it doesn't\n","        if (source, title) not in source_with_citations:\n","            source_with_citations[(source, title)] = {\n","                'title': title,\n","                'source': source,\n","                'citations': []\n","            }\n","\n","        # Find or create the citation entry for this unique (id, page) combination\n","        citation_entry = next(\n","            (c for c in source_with_citations[(source, title)]['citations'] if c['id'] == cite_id and c['page'] == page),\n","            None\n","        )\n","        if citation_entry is None:\n","            citation_entry = {'id': cite_id, 'page': page, 'quote': [quote], 'context': None}\n","            source_with_citations[(source, title)]['citations'].append(citation_entry)\n","        else:\n","            citation_entry['quote'].append(quote)\n","\n","    # Process context data\n","    for context in result_obj['context']:\n","        context_id = context.metadata['id']\n","        context_page = context.metadata['page']\n","        source = context.metadata['source']\n","        title = context.metadata['title']\n","        page_content = context.page_content\n","\n","        # Match the context to the correct citation entry by source, title, id, and page\n","        if (source, title) in source_with_citations:\n","            for citation in source_with_citations[(source, title)]['citations']:\n","                if citation['id'] == context_id and citation['page'] == context_page:\n","                    # Apply highlighting for each quote in the citation's quote list\n","                    highlighted_content = page_content\n","                    for quote in citation['quote']:\n","                        highlighted_content = highlight_text(highlighted_content, quote)\n","                    citation['context'] = highlighted_content\n","\n","    # Convert the dictionary to a list of dictionaries for separate entries\n","    final_result_list = [\n","        {\n","            'title': details['title'],\n","            'source': details['source'],\n","            'citations': details['citations']\n","        }\n","        for details in source_with_citations.values()\n","    ]\n","\n","    return final_result_list\n"],"metadata":{"id":"docbPBPDxSVa","executionInfo":{"status":"ok","timestamp":1752497430101,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["get_cited_context(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kq8XyKnlJb4i","outputId":"4f32040d-fa86-479e-f459-5f11af332a4c","executionInfo":{"status":"ok","timestamp":1752497430116,"user_tz":-330,"elapsed":11,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":74,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-73-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  for cite in result_obj['citations'].dict()['citations']:\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'title': 'Machine learning',\n","  'source': 'Wikipedia',\n","  'citations': [{'id': '564928',\n","    'page': 1,\n","    'quote': ['Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'],\n","    'context': 'Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. **They build a model from sample inputs**. **Machine learning is done where designing and programming explicit algorithms cannot be done**. **Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision**.'}]}]"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_results(result_obj):\n","    print('Query:')\n","    display(Markdown(result_obj['question']))\n","    print()\n","    print('Response:')\n","    display(Markdown(result_obj['answer']))\n","    print('='*50)\n","    print('Sources:')\n","    cited_context = get_cited_context(result_obj)\n","    for source in cited_context:\n","        print('Title:', source['title'], ' ', 'Source:', source['source'])\n","        print('Citations:')\n","        for citation in source['citations']:\n","            print('ID:', citation['id'], ' ', 'Page:', citation['page'])\n","            print('Cited Quotes:')\n","            display(Markdown('*'+' '.join(citation['quote'])+'*'))\n","            print('Cited Context:')\n","            display(Markdown(citation['context']))\n","            print()\n"],"metadata":{"id":"aChUXboG903B","executionInfo":{"status":"ok","timestamp":1752497430122,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":821},"id":"pG6fcxAE3I3G","outputId":"436784ab-9fb1-4449-8b3e-2369f1d46f46","executionInfo":{"status":"ok","timestamp":1752497430135,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is machine learning"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in the broader field of artificial intelligence (AI). \n\nMachine learning involves the study and construction of algorithms that can learn from and make predictions based on data. These algorithms follow programmed instructions but can also make decisions or predictions based on the data they process. The process typically involves building a model from sample inputs, which allows the system to operate in scenarios where designing and programming explicit algorithms is not feasible.\n\nSome common applications of machine learning include:\n- Spam filtering\n- Detection of network intruders or malicious insiders\n- Optical character recognition (OCR)\n- Search engines\n- Computer vision\n\nOverall, machine learning enables systems to improve their performance on tasks over time as they are exposed to more data, making it a powerful tool in various domains."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: Machine learning   Source: Wikipedia\n","Citations:\n","ID: 564928   Page: 1\n","Cited Quotes:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-73-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  for cite in result_obj['citations'].dict()['citations']:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. **They build a model from sample inputs**. **Machine learning is done where designing and programming explicit algorithms cannot be done**. **Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision**."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is AI, ML and DL?\"\n","result = rag_chain_w_citations.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rgTlW5hg_d0x","outputId":"a7eba569-da1b-49b1-a3ed-e344a75f73dc","executionInfo":{"status":"ok","timestamp":1752497444461,"user_tz":-330,"elapsed":14325,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is AI, ML and DL?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Artificial Intelligence (AI)**: AI is defined as the ability of a computer program or machine to think and learn. It is also a field of study aimed at making computers \"smart,\" allowing them to operate independently without being explicitly programmed with commands. The term was coined by John McCarthy in 1955. AI encompasses systems that can interpret external data, learn from it, and adapt to achieve specific goals. As technology advances, tasks once considered to require intelligence, such as optical character recognition, are no longer classified as AI but rather as routine technologies.\n\n**Machine Learning (ML)**: ML is a subfield of computer science that provides computers the ability to learn from data without being explicitly programmed. This concept emerged from the broader field of artificial intelligence. Machine learning involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. It is particularly useful in scenarios where traditional programming is impractical. Examples of machine learning applications include spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.\n\n**Deep Learning (DL)**: DL is a specialized form of machine learning that primarily utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). It can involve unsupervised, semi-supervised, or supervised learning sessions. Deep learning is particularly effective for complex tasks such as speech recognition, image understanding, and handwriting recognition, which are challenging for computers. The architecture of deep learning models is inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: Artificial intelligence   Source: Wikipedia\n","Citations:\n","ID: 6360   Page: 1\n","Cited Quotes:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-73-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  for cite in result_obj['citations'].dict()['citations']:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn**. It is also a field of study which tries to make computers \"smart\". **They work on their own without being encoded with commands**. **John McCarthy came up with the name \"Artificial Intelligence\" in 1955**. **In general use, the term \"artificial intelligence\" means a programme which mimics human cognition**. **At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do**. **Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation**. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Machine learning   Source: Wikipedia\n","Citations:\n","ID: 564928   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Deep learning   Source: Wikipedia\n","Citations:\n","ID: 663523   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks**. **As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised**. **In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer**. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"How is Machine learning related to supervised learning and clustering?\"\n","result = rag_chain_w_citations.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Zho3RfTQ-_WU","outputId":"1d0380b9-f3bb-404a-ba22-a78aac7b6f06","executionInfo":{"status":"ok","timestamp":1752497456295,"user_tz":-330,"elapsed":11829,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"How is Machine learning related to supervised learning and clustering?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a broad field that encompasses various techniques and methodologies for enabling computers to learn from data. Within this field, two important concepts are supervised learning and clustering.\n\n### Supervised Learning\nSupervised learning is a specific type of machine learning where the algorithm is trained on labeled data. This means that the training dataset includes both the input data and the corresponding correct outputs (labels). The primary goal of supervised learning is to infer a function that can map inputs to the correct outputs based on the training data. The system learns to produce a \"classifier\" that can make predictions on new, unseen data by generalizing from the training examples. This approach relies on inductive reasoning to derive patterns from the labeled data.\n\n### Clustering\nClustering, or cluster analysis, is another technique within the realm of machine learning, but it falls under the category of unsupervised learning. In clustering, the objective is to group a set of objects in such a way that objects within the same group (or cluster) are more similar to each other than to those in other groups. Unlike supervised learning, clustering does not use labeled data; instead, it seeks to identify inherent structures or patterns in the data based solely on the features of the objects being analyzed.\n\n### Relationship Between Machine Learning, Supervised Learning, and Clustering\n- **Machine Learning**: This is the overarching field that includes various learning paradigms, including both supervised and unsupervised learning techniques.\n- **Supervised Learning**: A subset of machine learning focused on learning from labeled data to make predictions or classifications.\n- **Clustering**: A technique within machine learning that involves grouping data without prior labels, focusing on the similarities among data points.\n\nIn summary, supervised learning and clustering are both integral parts of machine learning, each serving different purposes and utilizing different types of data. Supervised learning relies on labeled data to train models for prediction, while clustering seeks to discover patterns in unlabeled data by grouping similar items together."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: Supervised learning   Source: Wikipedia\n","Citations:\n","ID: 359370   Page: 1\n","Cited Quotes:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-73-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  for cite in result_obj['citations'].dict()['citations']:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**In machine learning, supervised learning is the task of inferring a function from labelled training data**. **The results of the training are known beforehand, the system simply learns how to get to these results correctly**. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Machine learning   Source: Wikipedia\n","Citations:\n","ID: 564928   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Machine learning gives computers the ability to learn without being explicitly programmed. It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Machine learning gives computers the ability to learn without being explicitly programmed** (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Cluster analysis   Source: Wikipedia\n","Citations:\n","ID: 593732   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Clustering or cluster analysis is a type of data analysis**. **The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way**. This is a common task in data mining."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is the difference between transformers and vision transformers?\"\n","result = rag_chain_w_citations.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vy2zTEfAFxBo","outputId":"1b510eea-1435-4a40-8157-c51170e0ffba","executionInfo":{"status":"ok","timestamp":1752497471154,"user_tz":-330,"elapsed":14853,"user":{"displayName":"Vipin Vashist","userId":"10703359930675166685"}}},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is the difference between transformers and vision transformers?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between traditional Transformers and Vision Transformers (ViTs) primarily lies in their application and the way they process input data.\n\n### Traditional Transformers\n- **Application**: Originally designed for natural language processing (NLP) tasks, traditional Transformers operate on sequences of tokens, such as words in a sentence.\n- **Input Representation**: They take a sequence of embeddings as input, where each token (word) is represented by a vector. The architecture relies heavily on self-attention mechanisms to capture relationships between tokens in the sequence.\n- **Inductive Bias**: Transformers do not inherently possess inductive biases like locality or translation equivariance, which are common in convolutional neural networks (CNNs). This can lead to challenges when applied to tasks with limited data.\n\n### Vision Transformers (ViTs)\n- **Application**: ViTs adapt the Transformer architecture for image classification tasks by treating image patches as tokens. This allows them to directly apply the Transformer model to visual data.\n- **Input Representation**: Images are divided into fixed-size patches, which are then flattened and linearly embedded into a sequence of vectors. These patch embeddings are treated similarly to word embeddings in NLP.\n- **Position Embeddings**: ViTs incorporate position embeddings to retain spatial information about the arrangement of patches, which is crucial for understanding the structure of images.\n- **Performance**: When pre-trained on large datasets, ViTs can achieve competitive performance on various image recognition benchmarks, often outperforming traditional CNNs in terms of compute efficiency. They require fewer computational resources to achieve similar or better results compared to CNNs.\n\n### Summary\nIn essence, while traditional Transformers are optimized for sequential data like text, Vision Transformers extend this architecture to handle image data by treating image patches as sequences of tokens, thus leveraging the strengths of self-attention in a visual context."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: vision_transformer.pdf   Source: ./rag_docs/vision_transformer.pdf\n","Citations:\n","ID: 5f2a682d-4ec5-4dd4-9a64-bc314b90bc67   Page: 7\n","Cited Quotes:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-73-3913888139.py:31: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  for cite in result_obj['citations'].dict()['citations']:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers outperform ResNets in terms of compute efficiency and suggesting potential for further scaling efforts. Additionally, it discusses the performance of hybrid models in comparison to pure Vision Transformers. Published as a conference paper at ICLR 2021 4.4 SCALING STUDY We perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre- trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone). Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5 for details on computational costs). Detailed results per model are provided in Table 6 in the Ap- pendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the performance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu- tational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts. 4.5 INSPECTING VISION TRANSFORMER Input Attention Figure 6: Representative ex- amples of attention from the output token to the input space. See Appendix D.7 for details. To begin to understand how the Vision Transformer processes im- age data, we analyze its internal representations. The ﬁrst layer of the Vision Transformer linearly projects the ﬂattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin- cipal components of the the learned embedding ﬁlters. The com- ponents resemble plausible basis functions for a low-dimensional representation of the ﬁne structure within each patch. After the projection, a learned position embedding is added to the patch representations. Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position em- beddings, i.e. closer patches tend to have more similar position em- beddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology ex- plains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4). Self-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Speciﬁcally, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure 7, right). This “attention distance” is analogous to receptive ﬁeld size in CNNs. We ﬁnd that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ID: 4cc53577-e83f-4046-84b1-b132bd16b715   Page: 0\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks with fewer computational resources when pre-trained on large datasets.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the introduction of the Vision Transformer (ViT) architecture, which applies a standard Transformer model directly to image classification tasks by treating image patches as tokens. **It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks with fewer computational resources when pre-trained on large datasets**. Published as a conference paper at ICLR 2021 AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,† ∗equal technical contribution, †equal advising Google Research, Brain Team {adosovitskiy, neilhoulsby}@google.com ABSTRACT While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classiﬁcation tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub- stantially fewer computational resources to train.1 1 INTRODUCTION Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks to Transformers’ computational efﬁciency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance. In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet- like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Trans- former. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classiﬁcation in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these mod- els yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases 1Fine-tuning code and pre-trained models are available at https://github.com/ google-research/vision_transformer 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ID: af27625e-37c9-434c-a1d6-549c4e308ab7   Page: 2\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*To handle 2D images, we reshape the image into a sequence of flattened 2D patches, where the standard Transformer receives as input a 1D sequence of token embeddings.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture. Published as a conference paper at ICLR 2021 Transformer Encoder MLP Head Vision Transformer (ViT) * Linear Projection of Flattened Patches * Extra learnable [ cl ass] embedding 1 2 3 4 5 6 7 8 9 0 Patch + Position Embedding Class Bird Ball Car ... Embedded Patches Multi-Head Attention Norm MLP Norm + L x + Transformer Encoder Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable “classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017). 3 METHOD In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efﬁcient implementations – can be used almost out of the box. 3.1 VISION TRANSFORMER (VIT) An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a sequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size D through all of its layers, so we ﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings. Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed- ded patches (z0 0 = xclass), whose state at the output of the Transformer encoder (z0 L) serves as the image representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at- tached to z0 L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at ﬁne-tuning time. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed signiﬁcant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder. The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self- attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019). 3"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ID: e9cf138e-adc2-42ea-8d24-2ae76a5d032a   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the performance of the Vision Transformer (ViT) in image classification tasks, highlighting its ability to achieve state-of-the-art results when pre-trained on large datasets. It contrasts the inductive biases of convolutional neural networks (CNNs) with the advantages of large-scale training for ViT, demonstrating its effectiveness on various benchmarks, including ImageNet and CIFAR-100. Additionally, it references related work in the field of self-attention and image processing. Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufﬁcient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images). We ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks. 2 RELATED WORK Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be- come the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod- eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self- attention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efﬁciently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well. There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]}]}